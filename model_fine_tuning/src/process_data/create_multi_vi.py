import json
import re
import os
import time
import threading
from datasets import load_dataset, Dataset
from huggingface_hub import login
from dotenv import load_dotenv
import google.generativeai as genai
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime

# Prompt template cho single sample - r√µ r√†ng v√† ƒë·∫ßy ƒë·ªß
single_prompt_template = '''
QUY LU·∫¨T X·ª¨ L√ù CHO M·ªñI <input_i>:

1. Ph√¢n t√≠ch t·ª´ng sampel json input ƒë·ªÉ ƒë·∫øm s·ªë parts.
2. T·∫°o multi-turn data t√πy ch·ªânh theo s·ªë part c·ªßa t·ª´ng sample json input: (json_1, prompt_1, think_1 -> ... -> json_i, prompt_i, think_i)

CHI TI·∫æT QUY LU·∫¨T:
- T·∫°o json progressive: json_1 (ch·ªâ part_1), json_2 (part_1 + part_2), ..., json_n (part_1 + part_2 + ... + part_n) (part n√†o kh√¥ng c√≥ th√¨ kh√¥ng t·∫°o)
- M·ªói json ƒë·∫∑t trong tag <json_i></json_i>
- T·∫°o m√¥ t·∫£ user input cho t·ª´ng part trong tag <prompt_i></prompt_i> (m√¥ t·∫£ h√¨nh d·∫°ng, kh√¥ng c√≥ s·ªë li·ªáu k·ªπ thu·∫≠t) (m√¥ t·∫£ user ƒë·ªÉ t√πy ch·ªânh t·ª´ part tr∆∞·ªõc ƒë√≥ ƒë·ªÉ c√≥ part n√†y)
- T·∫°o suy lu·∫≠n 2 b∆∞·ªõc trong tag <think_i></think_i>:
  * B∆∞·ªõc 1: Suy lu·∫≠n c√°c th√†nh ph·∫ßn s·∫Ω c√≥ trong JSON d·ª±a tr√™n m√¥ t·∫£ ƒë∆∞·ª£c cung c·∫•p
  * B∆∞·ªõc 2: Ki·ªÉm tra logic, t√≠nh ƒë√∫ng ƒë·∫Øn v·ªÅ s·ªë h·ªçc, v√† th·ª±c hi·ªán c√°c s·ª≠a ƒë·ªïi (n·∫øu c·∫ßn thi·∫øt) t·ª´ B∆∞·ªõc 1

FORMAT OUTPUT CHO M·ªñI SAMPLE (t·∫°o t·ª´ng part m·ªôt, part n√†o kh√¥ng c√≥ th√¨ kh√¥ng t·∫°o):
<sample_i>
<json_1>[json v·ªõi part_1]</json_1>
<prompt_1>[m√¥ t·∫£ user ƒë·ªÉ t·∫°o part_1]</prompt_1>
<think_1>[suy lu·∫≠n 2 b∆∞·ªõc ƒë·ªÉ t·∫°o json_1]</think_1>
<json_2>[json v·ªõi part_1 + part_2]</json_2>
<prompt_2>[m√¥ t·∫£ user ƒë·ªÉ t√πy ch·ªânh part_1 ƒë·ªÉ c√≥ ƒë∆∞·ª£c part 1 + part 2]</prompt_2> (ch·ªâ t·∫°o n·∫øu c√≥ part_2)
<think_2>[suy lu·∫≠n 2 b∆∞·ªõc ƒë·ªÉ t·∫°o json_2]</think_2>
...ti·∫øp t·ª•c ƒë·∫øn json_n (n·∫øu c√≥) ...
</sample_i>

V√ç D·ª§ C·ª§ TH·ªÇ:
***
INPUT:
<input_1>
{"parts":{"part_1":{"coordinate_system":{"Euler Angles":[0.0,0.0,0.0],"Translation Vector":[0.0,0.0,0.1607]},"sketch":{"face_1":{"loop_1":{"line_1":{"Start Point":[0.0,0.0],"End Point":[0.3214,0.0]},"line_2":{"Start Point":[0.3214,0.0],"End Point":[0.3214,0.3214]},"line_3":{"Start Point":[0.3214,0.3214],"End Point":[0.0,0.3214]},"line_4":{"Start Point":[0.0,0.3214],"End Point":[0.0,0.0]}}}},"extrusion":{"extrude_depth_towards_normal":0.0804,"extrude_depth_opposite_normal":0.0804,"sketch_scale":0.3214,"operation":"NewBodyFeatureOperation"}},"part_2":{"coordinate_system":{"Euler Angles":[0.0,0.0,0.0],"Translation Vector":[0.3214,0.0,0.1607]},"sketch":{"face_1":{"loop_1":{"line_1":{"Start Point":[0.0,0.0],"End Point":[0.2679,0.0]},"line_2":{"Start Point":[0.2679,0.0],"End Point":[0.2679,0.3214]},"line_3":{"Start Point":[0.2679,0.3214],"End Point":[0.0,0.3214]},"line_4":{"Start Point":[0.0,0.3214],"End Point":[0.0,0.0]}}}},"extrusion":{"extrude_depth_towards_normal":0.0268,"extrude_depth_opposite_normal":0.0268,"sketch_scale":0.3214,"operation":"JoinFeatureOperation"}},"part_3":{"coordinate_system":{"Euler Angles":[0.0,0.0,0.0],"Translation Vector":[0.5893,0.0,0.1607]},"sketch":{"face_1":{"loop_1":{"arc_1":{"Start Point":[0.0,0.0],"Mid Point":[0.1607,0.1607],"End Point":[0.0,0.3214]},"line_1":{"Start Point":[0.0,0.3214],"End Point":[0.0,0.0]}}}},"extrusion":{"extrude_depth_towards_normal":0.0268,"extrude_depth_opposite_normal":0.0268,"sketch_scale":0.3214,"operation":"JoinFeatureOperation"}}}}
</input_1>

OUTPUT:

<sample_1>
<json_1>
{"parts":{"part_1":{"coordinate_system":{"Euler Angles":[0.0,0.0,0.0],"Translation Vector":[0.0,0.0,0.0]},"sketch":{"face_1":{"loop_1":{"line_1":{"Start Point":[0.0,0.0],"End Point":[0.75,0.0]},"line_2":{"Start Point":[0.75,0.0],"End Point":[0.75,0.2812]},"line_3":{"Start Point":[0.75,0.2812],"End Point":[0.0,0.2812]},"line_4":{"Start Point":[0.0,0.2812],"End Point":[0.0,0.0]}},"loop_2":{"circle_1":{"Center":[0.1716,0.1406],"Radius":0.0769}},"loop_3":{"circle_1":{"Center":[0.5784,0.1406],"Radius":0.0769}}},"face_2":{"loop_1":{"circle_1":{"Center":[0.1716,0.1406],"Radius":0.0769}},"loop_2":{"circle_1":{"Center":[0.1716,0.1406],"Radius":0.0675}}},"face_3":{"loop_1":{"circle_1":{"Center":[0.5784,0.1406],"Radius":0.0769}},"loop_2":{"circle_1":{"Center":[0.5784,0.1406],"Radius":0.0675}}}},"extrusion":{"extrude_depth_towards_normal":0.0562,"extrude_depth_opposite_normal":0.0,"sketch_scale":0.75,"operation":"NewBodyFeatureOperation"}}}}
</json_1>
<prompt_1>
T√¥i mu·ªën t·∫°o m·ªôt t·∫•m ph·∫≥ng h√¨nh ch·ªØ nh·∫≠t. Tr√™n t·∫•m ƒë√≥ c√≥ hai l·ªó tr√≤n v√† ·ªü hai m·∫∑t b√™n c·ªßa t·∫•m, t·∫°i v·ªã tr√≠ hai l·ªó ƒë√≥, t√¥i mu·ªën c√≥ hai r√£nh tr√≤n ƒë·ªìng t√¢m.
</prompt_1>
<think_1>
B∆∞·ªõc 1: Suy lu·∫≠n c√°c th√†nh ph·∫ßn s·∫Ω c√≥ trong JSON d·ª±a tr√™n m√¥ t·∫£ ƒë∆∞·ª£c cung c·∫•p
T·ª´ y√™u c·∫ßu "T√¥i mu·ªën t·∫°o m·ªôt t·∫•m ph·∫≥ng h√¨nh ch·ªØ nh·∫≠t. Tr√™n t·∫•m ƒë√≥ c√≥ hai l·ªó tr√≤n v√† ·ªü hai m·∫∑t b√™n c·ªßa t·∫•m, t·∫°i v·ªã tr√≠ hai l·ªó ƒë√≥, t√¥i mu·ªën c√≥ hai r√£nh tr√≤n ƒë·ªìng t√¢m", t√¥i s·∫Ω ph√¢n t√≠ch v√† gi·∫£ ƒë·ªãnh c√°c y·∫øu t·ªë c·∫ßn thi·∫øt, li·ªát k√™ c√°c kh√≥a JSON c·ª• th·ªÉ.

**parts**: Kh√≥a c·∫•p cao nh·∫•t ƒë·ªÉ ch·ª©a c√°c ph·∫ßn c·ªßa m√¥ h√¨nh.
**part_1**: ƒê·ªãnh nghƒ©a ph·∫ßn ƒë·∫ßu ti√™n c·ªßa m√¥ h√¨nh.
**coordinate_system**: Th√†nh ph·∫ßn ƒë·ªÉ ƒë·ªãnh v·ªã v√† h∆∞·ªõng ph·∫ßn n√†y trong kh√¥ng gian.
  **Euler Angles**: T√¥i s·∫Ω gi·∫£ ƒë·ªãnh kh√¥ng c√≥ s·ª± xoay ban ƒë·∫ßu. T√¥i ƒë·∫∑t l√† `[0.0,0.0,0.0]`.
  **Translation Vector**: T√¥i s·∫Ω gi·∫£ ƒë·ªãnh m·ªôt v·ªã tr√≠ m·∫∑c ƒë·ªãnh t·∫°i g·ªëc t·ªça ƒë·ªô. T√¥i ƒë·∫∑t l√† `[0.0,0.0,0.0]`.
**sketch**: Th√†nh ph·∫ßn ƒë·ªãnh nghƒ©a b·∫£n ph√°c th·∫£o 2D c∆° s·ªü.
  **face_1**: ƒê·∫°i di·ªán cho m·∫∑t ph·∫≥ng ch·ª©a b·∫£n ph√°c th·∫£o ch√≠nh c·ªßa t·∫•m v√† c√°c l·ªó.
    **loop_1**: ƒê·∫°i di·ªán cho h√¨nh ch·ªØ nh·∫≠t b√™n ngo√†i c·ªßa t·∫•m.
      **line_1, line_2, line_3, line_4**: T√¥i s·∫Ω ƒë·∫∑t c√°c ƒëi·ªÉm `Start Point` v√† `End Point` ƒë·ªÉ t·∫°o h√¨nh ch·ªØ nh·∫≠t. V√≠ d·ª•: `line_1:{"Start Point":[0.0,0.0],"End Point":[0.75,0.0]}`, `line_2:{"Start Point":[0.75,0.0],"End Point":[0.75,0.2812]}`, `line_3:{"Start Point":[0.75,0.2812],"End Point":[0.0,0.2812]}`, `line_4:{"Start Point":[0.0,0.2812],"End Point":[0.0,0.0]}`.
    **loop_2, loop_3**: ƒê·∫°i di·ªán cho hai l·ªó tr√≤n tr√™n t·∫•m.
      **circle_1**: L√† h√¨nh d·∫°ng l·ªó tr√≤n.
        **Center**: T√¥i s·∫Ω ƒë·∫∑t c√°c v·ªã tr√≠ t√¢m cho hai l·ªó, v√≠ d·ª•: `loop_2:{"circle_1":{"Center":[0.1716,0.1406]}}` v√† `loop_3:{"circle_1":{"Center":[0.5784,0.1406]}}`.
        **Radius**: T√¥i s·∫Ω ƒë·∫∑t b√°n k√≠nh cho hai l·ªó, v√≠ d·ª•: `0.0769`.
  **face_2, face_3**: ƒê·∫°i di·ªán cho hai r√£nh tr√≤n ƒë·ªìng t√¢m ·ªü m·∫∑t b√™n. M·ªói r√£nh s·∫Ω l√† m·ªôt `face` ri√™ng.
    **loop_1**: ƒê·∫°i di·ªán cho v√≤ng ngo√†i c·ªßa r√£nh.
      **circle_1**: L√† h√¨nh d·∫°ng v√≤ng ngo√†i.
        **Center**: T√¥i s·∫Ω ƒë·∫∑t t√¢m c·ªßa v√≤ng ngo√†i tr√πng v·ªõi t√¢m l·ªó t∆∞∆°ng ·ª©ng, v√≠ d·ª•: `face_2:{"loop_1":{"circle_1":{"Center":[0.1716,0.1406]}}}` v√† `face_3:{"loop_1":{"circle_1":{"Center":[0.5784,0.1406]}}}`.
        **Radius**: T√¥i s·∫Ω ƒë·∫∑t b√°n k√≠nh c·ªßa v√≤ng ngo√†i, v√≠ d·ª•: `0.0769`.
    **loop_2**: ƒê·∫°i di·ªán cho v√≤ng trong c·ªßa r√£nh.
      **circle_1**: L√† h√¨nh d·∫°ng v√≤ng trong.
        **Center**: T√¥i s·∫Ω ƒë·∫∑t t√¢m c·ªßa v√≤ng trong tr√πng v·ªõi t√¢m v√≤ng ngo√†i, v√≠ d·ª•: `face_2:{"loop_2":{"circle_1":{"Center":[0.1716,0.1406]}}}` v√† `face_3:{"loop_2":{"circle_1":{"Center":[0.5784,0.1406]}}}`.
        **Radius**: T√¥i s·∫Ω ƒë·∫∑t b√°n k√≠nh c·ªßa v√≤ng trong, nh·ªè h∆°n v√≤ng ngo√†i m·ªôt ch√∫t, v√≠ d·ª•: `0.0675`.
**extrusion**: Th√†nh ph·∫ßn ƒë·ªãnh nghƒ©a qu√° tr√¨nh ƒë√πn ƒë·ªÉ t·∫°o h√¨nh 3D.
  **extrude_depth_towards_normal**: T√¥i s·∫Ω ƒë·∫∑t m·ªôt chi·ªÅu d√†y h·ª£p l√Ω cho t·∫•m, v√≠ d·ª• `0.0562`.
  **extrude_depth_opposite_normal**: T√¥i suy lu·∫≠n r·∫±ng qu√° tr√¨nh ƒë√πn ch·ªâ di·ªÖn ra m·ªôt chi·ªÅu. T√¥i ƒë·∫∑t l√† `0.0`.
  **sketch_scale**: T√¥i s·∫Ω ƒë·∫∑t m·ªôt t·ª∑ l·ªá ƒë·ªÉ t·∫•m kh√¥ng b·ªã thon, v√≠ d·ª• `0.75`.
  **operation**: T√¥i suy lu·∫≠n r·∫±ng ƒë√¢y l√† thao t√°c ƒë·ªÉ t·∫°o m·ªôt kh·ªëi 3D m·ªõi ho√†n ch·ªânh. T√¥i ƒë·∫∑t l√† `"NewBodyFeatureOperation"`.

B∆∞·ªõc 2: Ki·ªÉm tra logic, t√≠nh ƒë√∫ng ƒë·∫Øn v·ªÅ s·ªë h·ªçc, v√† th·ª±c hi·ªán c√°c s·ª≠a ƒë·ªïi (n·∫øu c·∫ßn thi·∫øt) t·ª´ B∆∞·ªõc 1
T√¥i ki·ªÉm tra t√≠nh h·ª£p l√Ω c·ªßa c√°c th√¥ng s·ªë ƒë√£ suy lu·∫≠n t·ª´ B∆∞·ªõc 1.

**coordinate_system**:
  **Euler Angles:[0.0,0.0,0.0]**: Suy lu·∫≠n: Kh√¥ng xoay. K·∫øt lu·∫≠n: H·ª£p l√Ω.
  **Translation Vector:[0.0,0.0,0.0]**: Suy lu·∫≠n: T·∫°i g·ªëc t·ªça ƒë·ªô. K·∫øt lu·∫≠n: H·ª£p l√Ω.
**sketch**:
  **face_1**:
    **loop_1 (h√¨nh ch·ªØ nh·∫≠t)**: C√°c ƒëi·ªÉm t·ªça ƒë·ªô t·∫°o h√¨nh ch·ªØ nh·∫≠t h·ª£p l√Ω. K·∫øt lu·∫≠n: H·ª£p l√Ω.
    **loop_2, loop_3 (l·ªó tr√≤n)**: T√¢m v√† b√°n k√≠nh c·ªßa c√°c l·ªó ƒë∆∞·ª£c ƒë·∫∑t ƒë·ªëi x·ª©ng v√† h·ª£p l√Ω tr√™n t·∫•m. K·∫øt lu·∫≠n: H·ª£p l√Ω.
  **face_2, face_3 (r√£nh ƒë·ªìng t√¢m)**:
    **loop_1, loop_2**: C√°c v√≤ng tr√≤n ƒë·ªìng t√¢m v·ªõi b√°n k√≠nh kh√°c nhau t·∫°o r√£nh. V·ªã tr√≠ t√¢m tr√πng v·ªõi c√°c l·ªó tr√™n `face_1` l√† h·ª£p l√Ω cho m√¥ t·∫£ "·ªü hai m·∫∑t b√™n c·ªßa t·∫•m, t·∫°i v·ªã tr√≠ hai l·ªó ƒë√≥". K·∫øt lu·∫≠n: H·ª£p l√Ω.
**extrusion**:
  **extrude_depth_towards_normal:0.0562**: Suy lu·∫≠n: X√°c ƒë·ªãnh chi·ªÅu d√†y c·ªßa t·∫•m. K·∫øt lu·∫≠n: H·ª£p l√Ω.
  **extrude_depth_opposite_normal:0.0**: Suy lu·∫≠n: Ch·ªâ ƒë√πn m·ªôt chi·ªÅu. K·∫øt lu·∫≠n: H·ª£p l√Ω.
  **sketch_scale:0.75**: Suy lu·∫≠n: T·ª∑ l·ªá ph√°c th·∫£o ƒë·ªÉ t·∫•m kh√¥ng b·ªã thon. K·∫øt lu·∫≠n: H·ª£p l√Ω.
  **operation:"NewBodyFeatureOperation"**: Suy lu·∫≠n: T·∫°o kh·ªëi m·ªõi. K·∫øt lu·∫≠n: H·ª£p l√Ω.
Th·ª±c hi·ªán c√°c s·ª≠a ƒë·ªïi (n·∫øu c·∫ßn thi·∫øt): Kh√¥ng c√≥ th√¥ng s·ªë n√†o c·∫ßn s·ª≠a ƒë·ªïi.
</think_1>

<json_2>
{"parts":{"part_1":{"coordinate_system":{"Euler Angles":[0.0,0.0,0.0],"Translation Vector":[0.0,0.0,0.0]},"sketch":{"face_1":{"loop_1":{"line_1":{"Start Point":[0.0,0.0],"End Point":[0.75,0.0]},"line_2":{"Start Point":[0.75,0.0],"End Point":[0.75,0.2812]},"line_3":{"Start Point":[0.75,0.2812],"End Point":[0.0,0.2812]},"line_4":{"Start Point":[0.0,0.2812],"End Point":[0.0,0.0]}},"loop_2":{"circle_1":{"Center":[0.1716,0.1406],"Radius":0.0769}},"loop_3":{"circle_1":{"Center":[0.5784,0.1406],"Radius":0.0769}}},"face_2":{"loop_1":{"circle_1":{"Center":[0.1716,0.1406],"Radius":0.0769}},"loop_2":{"circle_1":{"Center":[0.1716,0.1406],"Radius":0.0675}}},"face_3":{"loop_1":{"circle_1":{"Center":[0.5784,0.1406],"Radius":0.0769}},"loop_2":{"circle_1":{"Center":[0.5784,0.1406],"Radius":0.0675}}}},"extrusion":{"extrude_depth_towards_normal":0.0562,"extrude_depth_opposite_normal":0.0,"sketch_scale":0.75,"operation":"NewBodyFeatureOperation"}},"part_2":{"coordinate_system":{"Euler Angles":[0.0,0.0,0.0],"Translation Vector":[0.0947,0.0637,0.0562]},"sketch":{"face_1":{"loop_1":{"circle_1":{"Center":[0.0769,0.0769],"Radius":0.0769}},"loop_2":{"circle_1":{"Center":[0.0769,0.0769],"Radius":0.0675}}},"face_2":{"loop_1":{"circle_1":{"Center":[0.0769,0.0769],"Radius":0.0675}}},"face_3":{"loop_1":{"circle_1":{"Center":[0.4837,0.0769],"Radius":0.0769}},"loop_2":{"circle_1":{"Center":[0.4837,0.0769],"Radius":0.0675}}},"face_4":{"loop_1":{"circle_1":{"Center":[0.4837,0.0769],"Radius":0.0675}}}},"extrusion":{"extrude_depth_towards_normal":0.0,"extrude_depth_opposite_normal":0.0187,"sketch_scale":0.5606,"operation":"CutFeatureOperation"}}}}
</json_2>
<prompt_2>
T√¥i mu·ªën th√™m hai r√£nh tr√≤n n·ªØa, ·ªü hai ph√≠a c√≤n l·∫°i c·ªßa t·∫•m h√¨nh ch·ªØ nh·∫≠t, n·∫±m g·∫ßn c√°c l·ªó tr√≤n ban ƒë·∫ßu. Hai r√£nh n√†y c≈©ng l√† d·∫°ng v√†nh khuy√™n v√† n√¥ng.
</prompt_2>
<think_2>
B∆∞·ªõc 1: Suy lu·∫≠n c√°c th√†nh ph·∫ßn s·∫Ω c√≥ trong JSON d·ª±a tr√™n m√¥ t·∫£ ƒë∆∞·ª£c cung c·∫•p
T·ª´ y√™u c·∫ßu "T√¥i mu·ªën th√™m hai r√£nh tr√≤n n·ªØa, ·ªü hai ph√≠a c√≤n l·∫°i c·ªßa t·∫•m h√¨nh ch·ªØ nh·∫≠t, n·∫±m g·∫ßn c√°c l·ªó tr√≤n ban ƒë·∫ßu. Hai r√£nh n√†y c≈©ng l√† d·∫°ng v√†nh khuy√™n v√† n√¥ng", t√¥i s·∫Ω ph√¢n t√≠ch v√† gi·∫£ ƒë·ªãnh c√°c y·∫øu t·ªë c·∫ßn thi·∫øt, li·ªát k√™ c√°c kh√≥a JSON c·ª• th·ªÉ cho `part_2`.

**part_2**: ƒê·ªãnh nghƒ©a hai r√£nh tr√≤n m·ªõi.
**coordinate_system**: Th√†nh ph·∫ßn ƒë·ªÉ ƒë·ªãnh v·ªã v√† h∆∞·ªõng ph·∫ßn n√†y trong kh√¥ng gian.
  **Euler Angles**: T√¥i s·∫Ω gi·∫£ ƒë·ªãnh kh√¥ng c√≥ s·ª± xoay ban ƒë·∫ßu. T√¥i ƒë·∫∑t l√† `[0.0,0.0,0.0]`.
  **Translation Vector**: T√¥i s·∫Ω ƒë·∫∑t v·ªã tr√≠ c·ªßa c√°c r√£nh. Z-coordinate s·∫Ω b·∫±ng chi·ªÅu d√†y c·ªßa t·∫•m (`0.0562`) ƒë·ªÉ ch√∫ng n·∫±m tr√™n m·∫∑t tr√™n c·ªßa t·∫•m. X v√† Y s·∫Ω ƒë∆∞·ª£c d·ªãch m·ªôt ch√∫t ƒë·ªÉ c√°c r√£nh n·∫±m g·∫ßn c√°c l·ªó tr√≤n ban ƒë·∫ßu nh∆∞ng ·ªü ph√≠a kh√°c, v√≠ d·ª• `[0.0947,0.0637,0.0562]`.
**sketch**: Th√†nh ph·∫ßn ƒë·ªãnh nghƒ©a b·∫£n ph√°c th·∫£o 2D h√¨nh v√†nh khuy√™n (hai h√¨nh tr√≤n ƒë·ªìng t√¢m) cho m·ªói r√£nh.
  **face_1**: ƒê·∫°i di·ªán cho r√£nh th·ª© nh·∫•t.
    **loop_1**: ƒê·∫°i di·ªán cho v√≤ng ngo√†i c·ªßa r√£nh.
      **circle_1**: L√† h√¨nh d·∫°ng v√≤ng ngo√†i.
        **Center**: T√¥i s·∫Ω ƒë·∫∑t t√¢m c·ªßa v√≤ng ngo√†i, v√≠ d·ª• `[0.0769,0.0769]`.
        **Radius**: T√¥i s·∫Ω ƒë·∫∑t b√°n k√≠nh c·ªßa v√≤ng ngo√†i, v√≠ d·ª• `0.0769`.
    **loop_2**: ƒê·∫°i di·ªán cho v√≤ng trong c·ªßa r√£nh.
      **circle_1**: L√† h√¨nh d·∫°ng v√≤ng trong.
        **Center**: T√¥i s·∫Ω ƒë·∫∑t t√¢m c·ªßa v√≤ng trong tr√πng v·ªõi t√¢m v√≤ng ngo√†i, v√≠ d·ª• `[0.0769,0.0769]`.
        **Radius**: T√¥i s·∫Ω ƒë·∫∑t b√°n k√≠nh c·ªßa v√≤ng trong, v√≠ d·ª• `0.0675`.
  **face_2**: ƒê·∫°i di·ªán cho ph·∫ßn c·∫Øt c·ªßa r√£nh th·ª© nh·∫•t (n·∫øu c√≥ v√≤ng trong). T√¥i suy lu·∫≠n r·∫±ng ƒë√¢y l√† m·ªôt v√≤ng tr√≤n ƒë·ªìng t√¢m v·ªõi v√≤ng ngo√†i ƒë·ªÉ t·∫°o r√£nh v√†nh khuy√™n.
    **loop_1**:
      **circle_1**:
        **Center**: T√¥i s·∫Ω ƒë·∫∑t t√¢m c·ªßa v√≤ng trong tr√πng v·ªõi t√¢m v√≤ng ngo√†i, v√≠ d·ª• `[0.0769,0.0769]`.
        **Radius**: T√¥i s·∫Ω ƒë·∫∑t b√°n k√≠nh c·ªßa v√≤ng trong, v√≠ d·ª• `0.0675`.
  **face_3**: ƒê·∫°i di·ªán cho r√£nh th·ª© hai.
    **loop_1**: ƒê·∫°i di·ªán cho v√≤ng ngo√†i c·ªßa r√£nh.
      **circle_1**: L√† h√¨nh d·∫°ng v√≤ng ngo√†i.
        **Center**: T√¥i s·∫Ω ƒë·∫∑t t√¢m c·ªßa v√≤ng ngo√†i, v√≠ d·ª• `[0.4837,0.0769]`.
        **Radius**: T√¥i s·∫Ω ƒë·∫∑t b√°n k√≠nh c·ªßa v√≤ng ngo√†i, v√≠ d·ª• `0.0769`.
    **loop_2**: ƒê·∫°i di·ªán cho v√≤ng trong c·ªßa r√£nh.
      **circle_1**: L√† h√¨nh d·∫°ng v√≤ng trong.
        **Center**: T√¥i s·∫Ω ƒë·∫∑t t√¢m c·ªßa v√≤ng trong tr√πng v·ªõi t√¢m v√≤ng ngo√†i, v√≠ d·ª• `[0.4837,0.0769]`.
        **Radius**: T√¥i s·∫Ω ƒë·∫∑t b√°n k√≠nh c·ªßa v√≤ng trong, v√≠ d·ª• `0.0675`.
  **face_4**: ƒê·∫°i di·ªán cho ph·∫ßn c·∫Øt c·ªßa r√£nh th·ª© hai (n·∫øu c√≥ v√≤ng trong). T√¥i suy lu·∫≠n r·∫±ng ƒë√¢y l√† m·ªôt v√≤ng tr√≤n ƒë·ªìng t√¢m v·ªõi v√≤ng ngo√†i ƒë·ªÉ t·∫°o r√£nh v√†nh khuy√™n.
    **loop_1**:
      **circle_1**:
        **Center**: T√¥i s·∫Ω ƒë·∫∑t t√¢m c·ªßa v√≤ng trong tr√πng v·ªõi t√¢m v√≤ng ngo√†i, v√≠ d·ª• `[0.4837,0.0769]`.
        **Radius**: T√¥i s·∫Ω ƒë·∫∑t b√°n k√≠nh c·ªßa v√≤ng trong, v√≠ d·ª• `0.0675`.
**extrusion**: Th√†nh ph·∫ßn ƒë·ªãnh nghƒ©a qu√° tr√¨nh ƒë√πn ƒë·ªÉ t·∫°o r√£nh (c·∫Øt).
  **extrude_depth_towards_normal**: T√¥i suy lu·∫≠n r·∫±ng qu√° tr√¨nh kho√©t r√£nh s·∫Ω ƒëi ng∆∞·ª£c chi·ªÅu ph√°p tuy·∫øn c·ªßa b·ªÅ m·∫∑t. T√¥i ƒë·∫∑t l√† `0.0`.
  **extrude_depth_opposite_normal**: T√¥i s·∫Ω ƒë·∫∑t chi·ªÅu s√¢u c·ªßa r√£nh r·∫•t n√¥ng, v√≠ d·ª• `0.0187`.
  **sketch_scale**: T√¥i s·∫Ω ƒë·∫∑t m·ªôt t·ª∑ l·ªá ƒë·ªÉ r√£nh kh√¥ng b·ªã bi·∫øn d·∫°ng, v√≠ d·ª• `0.5606`.
  **operation**: T√¥i suy lu·∫≠n r·∫±ng ƒë√¢y l√† thao t√°c ƒë·ªÉ c·∫Øt v·∫≠t th·ªÉ hi·ªán c√≥. T√¥i ƒë·∫∑t l√† `"CutFeatureOperation"`.

B∆∞·ªõc 2: Ki·ªÉm tra logic, t√≠nh ƒë√∫ng ƒë·∫Øn v·ªÅ s·ªë h·ªçc, v√† th·ª±c hi·ªán c√°c s·ª≠a ƒë·ªïi (n·∫øu c·∫ßn thi·∫øt) t·ª´ B∆∞·ªõc 1
T√¥i ki·ªÉm tra t√≠nh h·ª£p l√Ω c·ªßa c√°c th√¥ng s·ªë ƒë√£ suy lu·∫≠n t·ª´ B∆∞·ªõc 1.

**coordinate_system**:
  **Euler Angles:[0.0,0.0,0.0]**: Suy lu·∫≠n: Kh√¥ng xoay. K·∫øt lu·∫≠n: H·ª£p l√Ω.
  **Translation Vector:[0.0947,0.0637,0.0562]**: Suy lu·∫≠n: V·ªã tr√≠ tr√™n m·∫∑t tr√™n c·ªßa t·∫•m v√† h∆°i d·ªãch chuy·ªÉn. K·∫øt lu·∫≠n: H·ª£p l√Ω.
**sketch**:
  **face_1, face_3 (v√≤ng ngo√†i)**:
    **circle_1**: T√¢m v√† b√°n k√≠nh t·∫°o v√≤ng ngo√†i. K·∫øt lu·∫≠n: H·ª£p l√Ω.
  **face_1, face_3 (v√≤ng trong)**:
    **circle_1**: T√¢m v√† b√°n k√≠nh t·∫°o v√≤ng trong ƒë·ªìng t√¢m v√† nh·ªè h∆°n. K·∫øt lu·∫≠n: H·ª£p l√Ω.
  **face_2, face_4**: C√°c r√£nh n√†y c√≥ v·∫ª ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a b·∫±ng m·ªôt v√≤ng tr√≤n duy nh·∫•t (ch·ªâ c√≥ `loop_1` v√† `circle_1`), nh∆∞ng m√¥ t·∫£ l√† "v√†nh khuy√™n". C√≥ th·ªÉ `loop_2` c·ªßa `face_1` v√† `face_3` ƒë√£ t·∫°o ra v√≤ng trong, v√† `face_2` v√† `face_4` ch·ªâ l√† c√°c b·∫£n sao c·ªßa v√≤ng trong ƒë√≥ ho·∫∑c l√† c√°c m·∫∑t ph·∫≥ng ph√°c th·∫£o cho vi·ªác c·∫Øt. D·ª±a v√†o JSON, `face_2` v√† `face_4` ch·ªâ c√≥ m·ªôt `loop` v·ªõi m·ªôt `circle` c√≥ b√°n k√≠nh `0.0675`, ƒë√¢y ch√≠nh l√† b√°n k√≠nh c·ªßa v√≤ng trong t·ª´ `face_1` v√† `face_3`. ƒêi·ªÅu n√†y cho th·∫•y `face_2` v√† `face_4` ƒë∆∞·ª£c d√πng ƒë·ªÉ ƒë·ªãnh nghƒ©a l·ªó c·∫Øt cho r√£nh v√†nh khuy√™n. K·∫øt lu·∫≠n: H·ª£p l√Ω ƒë·ªÉ t·∫°o r√£nh v√†nh khuy√™n.
**extrusion**:
  **extrude_depth_towards_normal:0.0**: Suy lu·∫≠n: Kh√¥ng ƒë√πn v·ªÅ ph√≠a ph√°p tuy·∫øn. K·∫øt lu·∫≠n: H·ª£p l√Ω.
  **extrude_depth_opposite_normal:0.0187**: Suy lu·∫≠n: Chi·ªÅu s√¢u r√£nh r·∫•t n√¥ng. K·∫øt lu·∫≠n: H·ª£p l√Ω.
  **sketch_scale:0.5606**: Suy lu·∫≠n: T·ª∑ l·ªá ph√°c th·∫£o ƒë·ªÉ r√£nh kh√¥ng b·ªã bi·∫øn d·∫°ng. K·∫øt lu·∫≠n: H·ª£p l√Ω.
  **operation:"CutFeatureOperation"**: Suy lu·∫≠n: Th·ª±c hi·ªán thao t√°c c·∫Øt. K·∫øt lu·∫≠n: H·ª£p l√Ω.
Th·ª±c hi·ªán c√°c s·ª≠a ƒë·ªïi (n·∫øu c·∫ßn thi·∫øt): C·∫•u tr√∫c c·ªßa `face_2` v√† `face_4` trong JSON c√≥ v·∫ª h∆°i th·ª´a n·∫øu m·ª•c ƒë√≠ch l√† t·∫°o r√£nh v√†nh khuy√™n. Th√¥ng th∆∞·ªùng, r√£nh v√†nh khuy√™n ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a b·ªüi hai v√≤ng tr√≤n ƒë·ªìng t√¢m trong c√πng m·ªôt `loop` ho·∫∑c `face`. Tuy nhi√™n, v·ªõi c·∫•u tr√∫c hi·ªán t·∫°i, n√≥ v·∫´n c√≥ th·ªÉ ho·∫°t ƒë·ªông n·∫øu ph·∫ßn m·ªÅm CAD hi·ªÉu r·∫±ng vi·ªác c·∫Øt d·ª±a tr√™n v√≤ng tr√≤n nh·ªè h∆°n s·∫Ω t·∫°o ra m·ªôt ph·∫ßn r·ªóng b√™n trong v√≤ng tr√≤n l·ªõn h∆°n. V·ªõi m·ª•c ƒë√≠ch suy lu·∫≠n t·ª´ m√¥ t·∫£, t√¥i s·∫Ω gi·∫£ ƒë·ªãnh ƒë√¢y l√† c√°ch ng∆∞·ªùi d√πng mu·ªën t·∫°o r√£nh v√†nh khuy√™n.
</think_2>
</sample_1>
***
'''

# Simplified error logging - only track failed sample IDs
error_log_lock = threading.Lock()
failed_sample_ids = []

def log_failed_sample_ids(sample_ids):
    """Log failed sample IDs in batch"""
    global failed_sample_ids
    with error_log_lock:
        failed_sample_ids.extend(sample_ids)

def save_error_log(dataset_name, split_name, start_index):
    """Save simple error log with failed sample IDs"""
    global failed_sample_ids
    with error_log_lock:
        if failed_sample_ids:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"failed_ids_{dataset_name.replace('/', '_')}_{split_name}_{start_index}_{timestamp}.json"
            
            error_data = {
                "dataset": dataset_name,
                "split": split_name,
                "start_index": start_index,
                "failed_count": len(failed_sample_ids),
                "failed_sample_ids": failed_sample_ids,
                "timestamp": timestamp
            }
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(error_data, f, indent=2, ensure_ascii=False)
            
            print(f"Failed IDs saved: {filename}")
            print(f"Total failed samples: {len(failed_sample_ids)}")
        else:
            print("No failed samples to log!")

# Prompt template cho batch processing (dynamic number of samples)
def create_batch_prompt_template(batch_size, single_template, samples_data):
    """Create batch prompt template with dynamic number of samples"""
    
    # Create input sections with actual data (no placeholders)
    input_sections = []
    for i in range(1, batch_size + 1):
        sample_data = samples_data.get(f'input_{i}', '')
        input_sections.append(rf"<input_{i}>\n{sample_data}\n</input_{i}>")
    
    # Create output format sections
    output_sections = []
    for i in range(1, batch_size + 1):
        output_sections.append(f"<sample_{i}>\noutput_{i}\n</sample_{i}>")
    
    template = f'''
T√¥i s·∫Ω cung c·∫•p cho b·∫°n {batch_size} sample JSON input v√† b·∫°n c·∫ßn x·ª≠ l√Ω t·ª´ng sample ri√™ng bi·ªát.

C√ÅC SAMPLES JSON INPUT:
{chr(10).join(input_sections)}

OUTPUT Y√äU C·∫¶U:
X·ª≠ l√Ω t·ª´ng sample JSON input theo v√≠ d·ª• b√™n d∆∞·ªõi v√† ƒë·∫∑t k·∫øt qu·∫£ v√†o tag t∆∞∆°ng ·ª©ng:

{chr(10).join(output_sections)}

L∆ØU √ù QUAN TR·ªåNG:
- M·ªói <sample_i> ch·ª©a ƒë·∫ßy ƒë·ªß: json, prompt, think
- T·∫•t c·∫£ tag ph·∫£i ƒë∆∞·ª£c t·∫°o nh∆∞ quy lu·∫≠t trong v√≠ d·ª• b√™n d∆∞·ªõi.

QUY LU·∫¨T V√Ä V√ç D·ª§ X·ª¨ L√ù:
{single_template}
'''
    return template

def load_environment():
    """Load environment variables"""
    load_dotenv()
    hf_token = os.getenv('HF_TOKEN')
    if not hf_token:
        raise ValueError("Please set HF_TOKEN environment variable")
    login(token=hf_token)

def extract_json_from_completion(completion_text):
    """Extract JSON from completion field"""
    try:
        json_match = re.search(r'<json>\s*(.*?)\s*</json>', completion_text, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
            return json.loads(json_str)
        return None
    except:
        return None

def count_parts_in_json(json_data):
    """Count number of parts in JSON"""
    if not json_data or 'parts' not in json_data:
        return 0
    
    parts = json_data['parts']
    part_count = 0
    for key in parts.keys():
        if re.match(r'part_\d+', key):
            part_count += 1
    
    return part_count

def survey_dataset(dataset_name, split_name, expected_parts=None):
    """Survey dataset to find maximum number of parts and check consistency"""
    print(f"Loading dataset {dataset_name} with split {split_name}...")
    
    dataset = load_dataset(dataset_name, split=split_name)
    print(f"Dataset loaded with {len(dataset)} samples")
    
    max_parts = 0
    part_counts = {}
    valid_samples = 0
    consistent_samples = 0
    
    for idx, sample in enumerate(dataset):
        completion = sample.get('completion', '')
        json_data = extract_json_from_completion(completion)
        
        if json_data:
            parts_count = count_parts_in_json(json_data)
            if parts_count > 0:
                valid_samples += 1
                max_parts = max(max_parts, parts_count)
                
                if parts_count in part_counts:
                    part_counts[parts_count] += 1
                else:
                    part_counts[parts_count] = 1
                
                # Check consistency with expected parts
                if expected_parts is not None and parts_count == expected_parts:
                    consistent_samples += 1
    
    print(f"\nSurvey Results:")
    print(f"Total samples: {len(dataset)}")
    print(f"Valid samples with JSON: {valid_samples}")
    print(f"Maximum number of parts found: {max_parts}")
    
    # Check consistency
    is_consistent = False
    if expected_parts is not None:
        print(f"\nExpected parts: {expected_parts}")
        print(f"Samples with expected parts: {consistent_samples}/{valid_samples}")
        
        if consistent_samples == valid_samples and len(part_counts) == 1 and expected_parts in part_counts:
            is_consistent = True
            print(f"‚úÖ Dataset is CONSISTENT - All samples have exactly {expected_parts} parts")
        else:
            print(f"‚ùå Dataset is NOT CONSISTENT")
            print(f"Part count distribution:")
            for count in sorted(part_counts.keys()):
                percentage = (part_counts[count] / valid_samples) * 100 if valid_samples > 0 else 0
                print(f"  {count} parts: {part_counts[count]} samples ({percentage:.1f}%)")
    else:
        print(f"\nPart count distribution:")
        for count in sorted(part_counts.keys()):
            percentage = (part_counts[count] / valid_samples) * 100 if valid_samples > 0 else 0
            print(f"  {count} parts: {part_counts[count]} samples ({percentage:.1f}%)")
    
    return max_parts, part_counts, is_consistent

def extract_tags_from_response(response_text, max_parts):
    """Extract json_n, prompt_n, think_n tags from Gemini response"""
    extracted_data = {}
    
    # Extract t·∫•t c·∫£ tag c√≥ th·ªÉ c√≥ (l√™n ƒë·∫øn max_parts + th√™m m·ªôt √≠t ƒë·ªÉ ƒë·∫£m b·∫£o)
    for i in range(1, max_parts + 5):  # Th√™m buffer
        # Extract json_n
        json_pattern = rf'<json_{i}>\s*(.*?)\s*</json_{i}>'
        json_match = re.search(json_pattern, response_text, re.DOTALL)
        if json_match:
            extracted_data[f'json_{i}'] = json_match.group(1).strip()
        
        # Extract prompt_n
        prompt_pattern = rf'<prompt_{i}>\s*(.*?)\s*</prompt_{i}>'
        prompt_match = re.search(prompt_pattern, response_text, re.DOTALL)
        if prompt_match:
            extracted_data[f'input_{i}'] = prompt_match.group(1).strip()
        
        # Extract think_n
        think_pattern = rf'<think_{i}>\s*(.*?)\s*</think_{i}>'
        think_match = re.search(think_pattern, response_text, re.DOTALL)
        if think_match:
            extracted_data[f'think_{i}'] = think_match.group(1).strip()
    
    return extracted_data

def extract_samples_from_batch_response(response_text, batch_size):
    """Extract individual sample responses from batch response"""
    samples = {}
    
    for i in range(1, batch_size + 1):  # Extract sample_1 to sample_batch_size
        sample_pattern = rf'<sample_{i}>\s*(.*?)\s*</sample_{i}>'
        sample_match = re.search(sample_pattern, response_text, re.DOTALL)
        if sample_match:
            sample_content = sample_match.group(1).strip()
            samples[f'sample_{i}'] = {
                'content': sample_content,
                'length': len(sample_content)
            }
    
    return samples

def process_batch_with_gemini(samples_batch, api_key, max_parts, batch_size, thread_id, start_sample_index):
    """Process a batch of samples with Gemini API"""
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.5-flash')
        
        # Get actual batch size (might be less than max batch_size for last batch)
        actual_batch_size = len(samples_batch)
        
        # Prepare input data for batch prompt
        input_data = {}
        for i, sample in enumerate(samples_batch, 1):
            input_data[f'input_{i}'] = sample['completion']
        
        # Create batch prompt with data embedded directly (no .format() needed)
        prompt = create_batch_prompt_template(actual_batch_size, single_prompt_template, input_data)
        
        response = model.generate_content(prompt)
        
        if not response or not hasattr(response, 'text'):
            return [None] * len(samples_batch)
            
        response_text = response.text
        
        if not response_text or len(response_text.strip()) == 0:
            return [None] * len(samples_batch)
        
        # Extract individual samples from batch response
        extracted_samples = extract_samples_from_batch_response(response_text, actual_batch_size)
        
        if not extracted_samples:
            return [None] * len(samples_batch)
        
        # Process each sample and extract tags
        results = []
        for i, sample in enumerate(samples_batch, 1):
            sample_key = f'sample_{i}'
            
            if sample_key in extracted_samples:
                sample_content = extracted_samples[sample_key]['content']
                sample_length = extracted_samples[sample_key]['length']
                
                # Extract tags from this sample's content
                extracted_data = extract_tags_from_response(sample_content, max_parts)
                extracted_data['new_length'] = sample_length
                
                results.append(extracted_data)
            else:
                # If sample not found, return empty result
                results.append({'new_length': 0})
        
        return results
            
    except Exception as e:
        return [None] * len(samples_batch)

def process_thread(dataset_samples, thread_id, api_key_index, max_parts, samples_per_thread, wait_seconds, start_idx, end_idx, batch_size):
    """Process samples in a thread using batch processing with intelligent timing"""
    
    # Load API key
    api_key = os.getenv(f'GEMINI_API_KEY_{api_key_index}')
    if not api_key:
        return []
    
    processed_data = []
    thread_failed_ids = []
    
    # Process samples in batches
    dataset_list = list(dataset_samples)
    total_samples = len(dataset_list)
    
    for batch_start in tqdm(range(0, total_samples, batch_size), desc=f"Thread {thread_id}"):
        batch_end = min(batch_start + batch_size, total_samples)
        samples_batch = dataset_list[batch_start:batch_end]
        actual_batch_size = len(samples_batch)
        
        # Calculate actual sample indices
        start_sample_index = start_idx + batch_start
        
        # Measure API call timing
        api_start_time = time.time()
        
        # Process batch with Gemini
        batch_results = process_batch_with_gemini(samples_batch, api_key, max_parts, batch_size, thread_id, start_sample_index)
        
        api_end_time = time.time()
        api_elapsed_time = api_end_time - api_start_time
        
        batch_failed_ids = []
        
        # Process each sample in the batch
        for idx, (sample, result) in enumerate(zip(samples_batch, batch_results)):
            # Extract sample ID 
            sample_id = sample.get('id', f'unknown_{start_sample_index + idx}')
            
            # Count parts in original completion
            original_json = extract_json_from_completion(sample['completion'])
            original_parts_count = count_parts_in_json(original_json) if original_json else 0
            
            # Create new sample with original data (always include, even if processing failed)
            new_sample = {
                'id': sample.get('id', f'unknown_{start_sample_index + idx}'),
                'original_completion': sample['completion']
            }
            
            # Track sample success/failure and count generated samples
            generated_samples_count = 0
            is_failed = False
            
            if result is None:
                is_failed = True
                new_sample['new_length'] = 0
                # Initialize all fields with empty strings for failed samples
                for i in range(1, max_parts + 1):
                    new_sample[f'input_{i}'] = ''
                    new_sample[f'think_{i}'] = ''
                    new_sample[f'json_{i}'] = ''
            else:
                # Add new_length field
                new_sample['new_length'] = result.get('new_length', 0)
                
                # Initialize all possible fields based on max_parts and count generated samples
                for i in range(1, max_parts + 1):
                    input_val = result.get(f'input_{i}', '')
                    think_val = result.get(f'think_{i}', '')
                    json_val = result.get(f'json_{i}', '')
                    
                    new_sample[f'input_{i}'] = input_val
                    new_sample[f'think_{i}'] = think_val
                    new_sample[f'json_{i}'] = json_val
                    
                    # Count if all three fields are non-empty (successful generation)
                    if input_val.strip() and think_val.strip() and json_val.strip():
                        generated_samples_count += 1
                
                # Check if generated correct number of parts or no output
                if generated_samples_count != original_parts_count or generated_samples_count == 0:
                    is_failed = True
            
            # Track failed sample ID
            if is_failed:
                batch_failed_ids.append(sample_id)
            
            processed_data.append(new_sample)
        
        # Log failed IDs for this batch
        if batch_failed_ids:
            thread_failed_ids.extend(batch_failed_ids)
        
        # Intelligent wait based on actual API time
        if batch_end < total_samples:  # Don't wait after last batch
            if api_elapsed_time < wait_seconds:
                remaining_wait = wait_seconds - api_elapsed_time
                time.sleep(remaining_wait)
    
    # Log all failed IDs for this thread
    if thread_failed_ids:
        log_failed_sample_ids(thread_failed_ids)
        print(f"Thread {thread_id}: {len(thread_failed_ids)} failed samples")
    
    return processed_data

def create_multi_turn_dataset(dataset_name, split_name, new_dataset_name, num_threads, samples_per_thread, wait_seconds, start_index=0, batch_size=5, expected_parts=None, api_key_start_index=1):
    """Main function to create multi-turn dataset"""
    
    # Load environment
    load_environment()
    
    # Survey dataset first
    print("Surveying dataset...")
    max_parts, distribution, is_consistent = survey_dataset(dataset_name, split_name, expected_parts)
    
    if not max_parts:
        print("No valid samples found!")
        return
    
    # Check if we should proceed with API processing
    if expected_parts is not None:
        if not is_consistent:
            print(f"\n‚ùå PROCESSING STOPPED!")
            print(f"Dataset is not consistent with expected {expected_parts} parts.")
            print(f"Cannot proceed with Gemini API processing.")
            print(f"Please choose a dataset with all samples having exactly {expected_parts} parts.")
            return
        else:
            print(f"\n‚úÖ Dataset validation passed!")
            print(f"All samples have exactly {expected_parts} parts. Proceeding with processing...")
    else:
        print(f"\n‚ö†Ô∏è No expected parts specified. Proceeding with max_parts={max_parts}...")
    
    # Calculate total fields: id + original_completion + new_length + (input_i + think_i + json_i) * max_parts  
    total_fields = 3 + (max_parts * 3)
    print(f"Creating dataset fields for up to {max_parts} parts")
    print(f"Dataset will have {total_fields} fields: 3 original + {max_parts}*3 = {total_fields} total fields")
    
    # Load dataset
    dataset = load_dataset(dataset_name, split=split_name)
    
    # Calculate samples per thread starting from start_index
    available_samples = len(dataset) - start_index
    total_samples = min(available_samples, num_threads * samples_per_thread)
    actual_samples_per_thread = total_samples // num_threads
    
    print(f"Dataset has {len(dataset)} samples, starting from index {start_index}")
    print(f"Processing {total_samples} samples with {num_threads} threads ({actual_samples_per_thread} samples per thread)")
    print(f"Using batch processing: {batch_size} samples per API call")
    print(f"API keys: GEMINI_API_KEY_{api_key_start_index} to GEMINI_API_KEY_{api_key_start_index + num_threads - 1}")
    
    # Split dataset for threads, starting from start_index
    thread_datasets = []
    for i in range(num_threads):
        thread_start_idx = start_index + (i * actual_samples_per_thread)
        thread_end_idx = min(start_index + ((i + 1) * actual_samples_per_thread), start_index + total_samples)
        thread_samples = dataset.select(range(thread_start_idx, thread_end_idx))
        thread_datasets.append((thread_samples, thread_start_idx, thread_end_idx))
    
    # Process with threads
    all_processed_data = []
    
    with ThreadPoolExecutor(max_workers=num_threads) as executor:
        # Create mapping of future to thread info
        future_to_info = {}
        
        for thread_id, (thread_samples, start_idx, end_idx) in enumerate(thread_datasets):
            api_key_index = api_key_start_index + thread_id  # Use consecutive API keys starting from api_key_start_index
            
            future = executor.submit(
                process_thread,
                thread_samples,
                thread_id,
                api_key_index,
                max_parts,
                actual_samples_per_thread,
                wait_seconds,
                start_idx,
                end_idx,
                batch_size
            )
            future_to_info[future] = (thread_id, start_idx, end_idx)
        
        # Collect results
        for future in as_completed(future_to_info.keys()):
            thread_id, start_idx, end_idx = future_to_info[future]
            try:
                thread_results = future.result()
                all_processed_data.extend(thread_results)
                print(f"Thread {thread_id} completed: {len(thread_results)} samples (samples {start_idx}-{end_idx})")
            except Exception as e:
                print(f"Thread {thread_id} failed with exception: {str(e)}")

    # Save error log
    save_error_log(dataset_name, split_name, start_index)
    
    if not all_processed_data:
        print("No data was processed successfully!")
        return

    print(f"Total processed samples: {len(all_processed_data)}")
    
    # Ensure all samples have the same structure before creating dataset
    if all_processed_data:
        # Create a template with all required fields
        field_template = {
            'id': '',
            'original_completion': '',
            'new_length': 0
        }
        for i in range(1, max_parts + 1):
            field_template[f'input_{i}'] = ''
            field_template[f'think_{i}'] = ''
            field_template[f'json_{i}'] = ''
        
        # Ensure all samples have all fields
        for sample in all_processed_data:
            for field_name, default_value in field_template.items():
                if field_name not in sample:
                    sample[field_name] = default_value
        
        print(f"Dataset structure validated. Each sample has {len(field_template)} fields.")
    
    # Create new dataset
    new_dataset = Dataset.from_list(all_processed_data)
    
    # Create split name based on consistency
    end_index = start_index + total_samples
    if is_consistent and expected_parts is not None:
        new_split_name = f"{split_name}_{start_index}_{end_index}_{expected_parts}parts"
        print(f"‚úÖ Dataset is consistent with {expected_parts} parts")
    else:
        new_split_name = f"{split_name}_{start_index}_{end_index}"
        if expected_parts is not None:
            print(f"‚ùå Dataset is not consistent with expected {expected_parts} parts")
        else:
            print(f"‚ÑπÔ∏è No expected parts specified")
    
    print(f"Final split name will be: {new_split_name}")
    print(f"Pushing dataset to {new_dataset_name} with split {new_split_name}")
    
    new_dataset.push_to_hub(new_dataset_name, split=new_split_name)
    print("Dataset pushed successfully!")
    print(f"üì§ Uploaded split: {new_split_name}")

# Example usage
if __name__ == "__main__":
    # Parameters
    dataset_name = "wanhin/cad_reasoning_part"
    split_name = "vi_part_1"
    new_dataset_name = "wanhin/output_reasoning_part"
    num_threads = 4
    samples_per_thread = 9000  # Total samples per thread
    wait_seconds = 70  # Minimum interval between API calls (intelligent wait - only waits if API call < this time)
    start_index = 72000  # Starting index parameter
    batch_size = 9  # Number of samples per batch/API call
    expected_parts = 1  # Expected number of parts in dataset (None if not checking)
    api_key_start_index = 6  # Start from GEMINI_API_KEY_1, GEMINI_API_KEY_2, etc.
    
    create_multi_turn_dataset(
        dataset_name=dataset_name,
        split_name=split_name,
        new_dataset_name=new_dataset_name,
        num_threads=num_threads,
        samples_per_thread=samples_per_thread,
        wait_seconds=wait_seconds,
        start_index=start_index,
        batch_size=batch_size,
        expected_parts=expected_parts,
        api_key_start_index=api_key_start_index
    )